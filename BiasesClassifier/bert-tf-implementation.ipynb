{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-29T04:55:06.043613Z","iopub.execute_input":"2021-07-29T04:55:06.043983Z","iopub.status.idle":"2021-07-29T04:55:06.054366Z","shell.execute_reply.started":"2021-07-29T04:55:06.043901Z","shell.execute_reply":"2021-07-29T04:55:06.053290Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/biases-dataset/ibc_data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:55:06.058435Z","iopub.execute_input":"2021-07-29T04:55:06.059104Z","iopub.status.idle":"2021-07-29T04:55:06.183990Z","shell.execute_reply.started":"2021-07-29T04:55:06.059060Z","shell.execute_reply":"2021-07-29T04:55:06.182973Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\n# clean text from noise\ndef clean_text(text):\n    # filter to allow only alphabets\n    text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n    \n    # remove Unicode characters\n    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n    \n    # convert to lowercase to maintain consistency\n    text = text.lower()\n       \n    return text\n\ndf['text'] = df.text.apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:55:06.212966Z","iopub.execute_input":"2021-07-29T04:55:06.213339Z","iopub.status.idle":"2021-07-29T04:55:06.431138Z","shell.execute_reply.started":"2021-07-29T04:55:06.213299Z","shell.execute_reply":"2021-07-29T04:55:06.429891Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nSEQ_LEN = 350\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\ndef tokenize(sentence):\n    tokens = tokenizer.encode_plus(sentence, max_length=SEQ_LEN,\n                                  truncation=True, padding='max_length',\n                                  add_special_tokens=True, return_attention_mask=True,\n                                  return_token_type_ids=False, return_tensors='tf')\n    return tokens['input_ids'], tokens['attention_mask']\n\n# initializing two array for imput tensors\nXids = np.zeros((len(df), SEQ_LEN))\nXmask = np.zeros((len(df), SEQ_LEN))\n\nfor i, sentence in enumerate(df['text']):\n    Xids[i, :], Xmask[i, :] = tokenize(sentence)\n    if i % 1000 == 0:\n        print(i)  # do this so we can see some progress","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:55:06.432917Z","iopub.execute_input":"2021-07-29T04:55:06.433466Z","iopub.status.idle":"2021-07-29T04:55:20.884404Z","shell.execute_reply.started":"2021-07-29T04:55:06.433430Z","shell.execute_reply":"2021-07-29T04:55:20.883154Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70ab802159c84e13822624ca6a08f986"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c967bacf01e49d981cd27a5e4be5d8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d5e8e578d6f49c39e80f70e70b6d7e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb66c7cb2df04e5e928c9330bbf41c2b"}},"metadata":{}},{"name":"stdout","text":"0\n1000\n2000\n3000\n4000\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nimport pickle\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['label'])\n\npickle_out = open(\"LabelEncoder.pickle\",\"wb\")\npickle.dump(le, pickle_out)\npickle_out.close()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:55:20.885869Z","iopub.execute_input":"2021-07-29T04:55:20.886265Z","iopub.status.idle":"2021-07-29T04:55:21.336946Z","shell.execute_reply.started":"2021-07-29T04:55:20.886210Z","shell.execute_reply":"2021-07-29T04:55:21.335965Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"arr = df['label'].values  # take sentiment column in df as array\nlabels = np.zeros((arr.size, arr.max()+1))  # initialize empty (all zero) label array\nlabels[np.arange(arr.size), arr] = 1  # add ones in indices where we have a value","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:55:21.339122Z","iopub.execute_input":"2021-07-29T04:55:21.339409Z","iopub.status.idle":"2021-07-29T04:55:21.347661Z","shell.execute_reply.started":"2021-07-29T04:55:21.339382Z","shell.execute_reply":"2021-07-29T04:55:21.346797Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\nwith open('biases-xids.npy', 'wb') as f:\n    np.save(f, Xids)\nwith open('biases-xmask.npy', 'wb') as f:\n    np.save(f, Xmask)\nwith open('biases-labels.npy', 'wb') as f:\n    np.save(f, labels)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:55:21.350895Z","iopub.execute_input":"2021-07-29T04:55:21.351152Z","iopub.status.idle":"2021-07-29T04:55:21.377848Z","shell.execute_reply.started":"2021-07-29T04:55:21.351128Z","shell.execute_reply":"2021-07-29T04:55:21.376948Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nBATCH_SIZE = 32\n\n# load arrays into tensorflow dataset\ndataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n\n# create a mapping function that we use to restructure our dataset\ndef map_func(input_ids, masks, labels):\n    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n\n# using map method to apply map_func to dataset\ndataset = dataset.map(map_func)\n# shuffle data and batch it\ndataset = dataset.shuffle(10000).batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:55:21.381681Z","iopub.execute_input":"2021-07-29T04:55:21.381963Z","iopub.status.idle":"2021-07-29T04:55:21.479924Z","shell.execute_reply.started":"2021-07-29T04:55:21.381935Z","shell.execute_reply":"2021-07-29T04:55:21.478921Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# get the length of the batched dataset\nDS_LEN = len([0 for batch in dataset])\nSPLIT = 0.8  # 80-20 split\n\ntrain = dataset.take(round(DS_LEN*SPLIT))  # get first 90% of batches\nval = dataset.skip(round(DS_LEN*SPLIT))  # skip first 90% and keep final 10%\n\ndel dataset  # optionally, delete dataset to free up disk-space","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:55:21.481596Z","iopub.execute_input":"2021-07-29T04:55:21.481956Z","iopub.status.idle":"2021-07-29T04:55:21.620320Z","shell.execute_reply.started":"2021-07-29T04:55:21.481924Z","shell.execute_reply":"2021-07-29T04:55:21.619470Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Model Definition","metadata":{}},{"cell_type":"code","source":"from transformers import TFAutoModel\n\nbert = TFAutoModel.from_pretrained('bert-base-uncased')\n\ninput_ids = tf.keras.layers.Input(shape=(350,), name='input_ids', dtype='int32')\nmask = tf.keras.layers.Input(shape=(350,), name='attention_mask', dtype='int32')\n\n# we consume the last_hidden_state tensor from bert (discarding pooled_outputs)\nembeddings = bert(input_ids, attention_mask=mask)[0]\n\nX = tf.keras.layers.LSTM(64)(embeddings)\nX = tf.keras.layers.BatchNormalization()(X)\nX = tf.keras.layers.Dense(64, activation='relu')(X)\nX = tf.keras.layers.Dropout(0.1)(X)\ny = tf.keras.layers.Dense(3, activation='softmax', name='outputs')(X)\n\n# define input and output layers of our model\nmodel = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n\n# freeze the BERT layer - otherwise we will be training 100M+ parameters...\nmodel.layers[2].trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:55:21.621719Z","iopub.execute_input":"2021-07-29T04:55:21.622080Z","iopub.status.idle":"2021-07-29T04:55:48.690676Z","shell.execute_reply.started":"2021-07-29T04:55:21.622043Z","shell.execute_reply":"2021-07-29T04:55:48.689697Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/536M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b27bb0001494a3dbff8cd27ed59e3c3"}},"metadata":{}},{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(0.001)\nloss = tf.keras.losses.CategoricalCrossentropy()  # categorical = one-hot\nacc = tf.keras.metrics.CategoricalAccuracy('accuracy')\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\"./Biases_Checkpoint\",\n                             monitor=\"accuracy\",\n                             mode=\"max\",\n                             save_best_only = True,\n                             verbose=1)\nearlystop = tf.keras.callbacks.EarlyStopping(monitor = 'accuracy', # value being monitored for improvement\n                          min_delta = 0, #Abs value and is the min change required before we stop\n                          patience = 5, #Number of epochs we wait before stopping \n                          verbose = 1,\n                          restore_best_weights = True)\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'accuracy', factor = 0.01, patience = 3,\n                                                 verbose = 1, min_delta = 0.0001)\n\ncallbacks = [checkpoint, earlystop, reduce_lr]\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n\nhistory = model.fit(train,\n                    validation_data=val,\n                    epochs=50,\n                    callbacks = callbacks)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:55:48.692116Z","iopub.execute_input":"2021-07-29T04:55:48.692739Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/50\n109/109 [==============================] - 88s 647ms/step - loss: 1.1055 - accuracy: 0.4398 - val_loss: 0.9556 - val_accuracy: 0.5477\n\nEpoch 00001: accuracy improved from -inf to 0.46359, saving model to ./Biases_Checkpoint\n","output_type":"stream"}]},{"cell_type":"code","source":"history_dict = history.history\naccuracy = max(history_dict['accuracy'])\nval_acc = 0.9893","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:36:31.932322Z","iopub.status.idle":"2021-07-29T04:36:31.933071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle \n\npickle_out = open(\"BIASES_history_{}_{}.pickle\".format(accuracy, val_acc),\"wb\")\npickle.dump(history.history, pickle_out)\npickle_out.close()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:36:31.934214Z","iopub.status.idle":"2021-07-29T04:36:31.934919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting our loss charts\nimport matplotlib.pyplot as plt\n\nhistory_dict = history.history\n\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\n\nline1 = plt.plot(epochs, val_loss_values, label='Validation/Test Loss')\nline2 = plt.plot(epochs, loss_values, label='Training Loss')\nplt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\nplt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\nplt.xlabel('Epochs') \nplt.ylabel('Loss')\nplt.grid(True)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:36:31.936080Z","iopub.status.idle":"2021-07-29T04:36:31.936903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting our accuracy charts\nimport matplotlib.pyplot as plt\n\nhistory_dict = history.history\n\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nepochs = range(1, len(loss_values) + 1)\n\nline1 = plt.plot(epochs, val_acc_values, label='Validation/Test Accuracy')\nline2 = plt.plot(epochs, acc_values, label='Training Accuracy')\nplt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\nplt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\nplt.xlabel('Epochs') \nplt.ylabel('Accuracy')\nplt.grid(True)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:36:31.938014Z","iopub.status.idle":"2021-07-29T04:36:31.938750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"./accuracy-{}-{}\".format(accuracy, val_acc))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:36:31.940099Z","iopub.status.idle":"2021-07-29T04:36:31.940943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n# shutil.make_archive(\"model\", 'zip', './accuracy-0.9601491093635559-0.9893')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:36:31.943069Z","iopub.status.idle":"2021-07-29T04:36:31.943792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = clean_text('But the most important step that Democrats can take is to transfer the fight for their agenda from the realm of pressure groups , where they are vulnerable , to the electorate , where they hold the advantage .')\nprint(model.predict(text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}